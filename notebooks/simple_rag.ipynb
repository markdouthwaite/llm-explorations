{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aee376271d614f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Getting started with RAG in Python\n",
    "\n",
    "This notebook aims to give a basic introduction to Retrieval-Augmented Generation (RAG) with GPT-4 in Python. The intent is to give an extemely transparent (if simple) runthrough of a basic RAG setup in Python. It is not intended to act as a technical reference for any production RAG-based systems. In such cases you should consider using a dedicated vector database (e.g. Qdrant, Chroma, Vespa) and some dedicated LLM tooling such as LangChain.\n",
    "\n",
    "Additionally, to use this notebook, you'll need an OpenAI API Key and billing set up for your OpenAI account.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To get started, make sure you're using Python 3.10 or greater. Install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce671fad498e247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-29T19:17:03.677905Z",
     "start_time": "2023-10-29T19:16:41.839356Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers<=4.34.0 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (4.34.0)\n",
      "Requirement already satisfied: openai in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: torch in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: filelock in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (3.13.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from transformers<=4.34.0) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from openai) (3.8.6)\n",
      "Requirement already satisfied: typing-extensions in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from requests->transformers<=4.34.0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from requests->transformers<=4.34.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from requests->transformers<=4.34.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from requests->transformers<=4.34.0) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/markdouthwaite/.virtualenvs/llm-explorations/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'transformers<=4.34.0' openai torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665c813952e9a7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, add your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85cc12df-587f-4be0-950a-e697d7348da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc2a97-50f3-4557-80bf-575a10e1f289",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "The basic RAG pattern involves two key components: the retriever and the generator. The generator is typically a LLM -- in this case it is going to be GPT-4. The retriever can be any external database, but is commonly centered on some form of vector database loaded with _embeddings_. However it is set up, the aim to have the retriever retrieve information in the form of relevant documents or snippets of documents from its data store and to use these to _augment_ the input (prompt) for the Generator in order to allow it to produce better responses. Sounds simple enough, right?\n",
    "\n",
    "\n",
    "## The data\n",
    "\n",
    "For this example, you'll use some short snippets about space missions in 2023. These were taken from [Wikipedia](https://en.wikipedia.org/wiki/2023_in_spaceflight) on 29th October 2023 with small modifications. At the time this notebook was written, GPT-4 had access to data up to October 2021. As such all of these events are outside of its parametric memory. Here are the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80dc3f38-1337-4401-90b6-2112ab7f5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "  \"On 14 April, ESA launched the Jupiter Icy Moons Explorer (JUICE) spacecraft to explore Jupiter and its large ice-covered moons following an eight-year transit.\",\n",
    "  \"ISRO launched its third lunar mission Chandrayaan-3 on 14 July 2023 at 9:05 UTC; it consists of lander, rover and a propulsion module, and successfully landed in the south pole region of the Moon on 23 August 2023.\",\n",
    "  \"Russian lunar lander Luna 25 was launched on 10 August 2023, 23:10 UTC, atop a Soyuz-2.1b rocket from the Vostochny Cosmodrome, it was the first Russian attempt to land a spacecraft on the Moon since the Soviet lander Luna 24 in 1974, it crashed on the Moon on 19 August after technical glitches.\",\n",
    "  \"JAXA launched SLIM (Smart Lander for Investigating Moon) lunar lander (carrying a mini rover) and a space telescope (XRISM) on 6 September.\",\n",
    "  \"The OSIRIS-REx mission returned to Earth on 24 September with samples collected from asteroid Bennu.\",\n",
    "  \"NASA launched the Psyche spacecraft on 13 October 2023, an orbiter mission that will explore the origin of planetary cores by studying the metallic asteroid 16 Psyche, on a Falcon Heavy launch vehicle.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0ad1e-2460-4e9a-ba5d-2ef2dfa1f4df",
   "metadata": {},
   "source": [
    "## The retriever\n",
    "\n",
    "In situations where you intend to use an off-the-shelf LLM (i.e. Generator), the retriever is the aspect of the system you have most control over. A common design for retrievers is to use a pre-trained language model to convert your documents into embeddings, and to then use a vector database to store these and query these embeddings during operation.\n",
    "\n",
    "In this simple example, you'll use a pre-trained embedding model available through the `transformers` library from [Hugging Face](huggingface.co). This [pre-trained model](https://huggingface.co/BAAI/bge-base-en) is the English language version of the 'General Embedding' model from Beijing Academy of Artificial Intelligence (BAAI). \n",
    "\n",
    "Here's a function that creates embeddings from a set of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ef6dc6-1b14-4023-b38a-abec745fd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def embed_documents(docs, model_name):\n",
    "  \"\"\"Embed the provided documents to create a document index\"\"\"\n",
    "  # load the tokenizer and model\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "  # encode the docs with the tokenizer\n",
    "  encoded_docs = tokenizer(\n",
    "      docs, padding=True, truncation=True, \n",
    "      return_tensors='pt'\n",
    "  )\n",
    "\n",
    "  # generate your output embedding vectors\n",
    "  with torch.no_grad():\n",
    "      model_output = model(**encoded_docs)\n",
    "      doc_embeddings = model_output[0][:, 0]\n",
    "  \n",
    "  # convert to numpy vectors for ease of use\n",
    "  return doc_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d4677-d44f-49b7-9098-b680052b403d",
   "metadata": {},
   "source": [
    "As you can see, there are two main elements here, the `tokenizer` and the `model`. You'll notice both use the same `model_name`. Language models often have their own tokenizers. This allows them to convert from human-readable natural language into machine-readable format compatible with the model. This machine-readable format is then used by the model to generate your embeddings.\n",
    "\n",
    "You can now generate your document index as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d855e0-9220-48cb-8a80-fe60b18686a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_index = embed_documents(documents, model_name=\"BAAI/bge-base-en\")\n",
    "document_index[0].shape # shape of each vector in the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116a37e-632e-42ea-8895-4dfbf1d02383",
   "metadata": {},
   "source": [
    "Note that it can be a good idea to 'chunk' your documents before embedding them and creating your document index. This can help reduce the context length subsequently required for the LLM you use, which in turn can improve inference speed and reduce cost. Additionally, it can help the resultant prompt reference specific facts or segments within a document more easily too, which can improve the quality of responses in some cases.\n",
    "\n",
    "Now that you have your document index, you can create a simple retrieval function to search the index for matching documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666e8f8-061c-4bbb-964a-f7d20d86fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def retrieve_documents(query_string, doc_index, docs, k=5, doc_model_name=\"BAAI/bge-base-en\"):\n",
    "  # embed the query string to obtain a query vector\n",
    "  query_vector = embed_documents(\n",
    "      [query_string], \n",
    "      model_name=\"BAAI/bge-base-en\"\n",
    "  ).reshape(1, -1)\n",
    "\n",
    "  # use the query vector to find the most similar document to the query\n",
    "  similarity = cosine_similarity(query_vector, doc_index).flatten()\n",
    "\n",
    "  # return the top k most similar docs\n",
    "  # here, argsort assigns the indices that would order the similarities\n",
    "  # from least similar to most similar. The [::-1] slice reverses this \n",
    "  # to return most similar to least, and slices the top k of these\n",
    "  return [docs[i] for i in np.argsort(similarity)[::-1][:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e8b51-68c0-403e-8831-1a00b8b53abc",
   "metadata": {},
   "source": [
    "In this case you're using [cosine similarity](https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity), a common method for comparing embedding vectors. The approach implemented here finds the similarity of your query vector to _every other document vector_. When you have lots of documents, this can be extremely expensive. It is in these circumstances that tools that provide _approximate_ search over the document index are useful. You can check out [FAISS](https://faiss.ai/index.html) from Facebook AI Research as an example of a tool that supports efficient similarity search over very large document indexes. For production environments, this is also where vector databases like [Qdrant](https://qdrant.tech/), [Chroma](https://www.trychroma.com/) or [Vespa](https://vespa.ai/) start to come in handy: they manage efficient similarity search for you!\n",
    "\n",
    "With all that said, you can then test this simple Retriever (i.e. `retrieve_documents`) using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54d253e9-db6d-4807-b770-46eb720f6dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JAXA launched SLIM (Smart Lander for Investigating Moon) lunar lander (carrying a mini rover) and a space telescope (XRISM) on 6 September.',\n",
       " 'ISRO launched its third lunar mission Chandrayaan-3 on 14 July 2023 at 9:05 UTC; it consists of lander, rover and a propulsion module, and successfully landed in the south pole region of the Moon on 23 August 2023.',\n",
       " 'Russian lunar lander Luna 25 was launched on 10 August 2023, 23:10 UTC, atop a Soyuz-2.1b rocket from the Vostochny Cosmodrome, it was the first Russian attempt to land a spacecraft on the Moon since the Soviet lander Luna 24 in 1974, it crashed on the Moon on 19 August after technical glitches.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_retrieved_docs = retrieve_documents(\n",
    "    \"Tell me about the Japanese lunar mission.\", \n",
    "    document_index,\n",
    "    documents,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "example_retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fcb15-8856-4f79-ac9b-e89a29f2fcd4",
   "metadata": {},
   "source": [
    "Clearly there are not many documents in this document index. However, you should see that the top document is indeed most relevant to the query text: exactly what you want to see! \n",
    "\n",
    "With this done, it is time to use the retrieved documents to create an augmented input for the LLM (i.e. create an augmented prompt). You'll use a very simple prompt in this case (you should think about ways to make it better!). Here's a simple function to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ab9bad-17e1-4989-b0fb-0292f8677d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmented_prompt(query_string, docs):\n",
    "  # concatenate the retrieved docs as context for the LLM\n",
    "  # you could do other pre-processing here too\n",
    "  context = \"\\n\".join(docs)\n",
    "  # define your prompt template\n",
    "  prompt_template = \"\"\"Here is some relevant information:\n",
    "  {context}\n",
    "\n",
    "  Q: {query}\n",
    "  A:\n",
    "  \"\"\"\n",
    "  # render the prompt template\n",
    "  return prompt_template.format(context=context, query=query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0c1b1-25e3-426a-a2d2-adc4027cc49f",
   "metadata": {},
   "source": [
    "And you can see how this behaves with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "822e59bd-45a6-48cb-a90b-5faaf10e67ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is some relevant information:\\n  JAXA launched SLIM (Smart Lander for Investigating Moon) lunar lander (carrying a mini rover) and a space telescope (XRISM) on 6 September.\\nISRO launched its third lunar mission Chandrayaan-3 on 14 July 2023 at 9:05 UTC; it consists of lander, rover and a propulsion module, and successfully landed in the south pole region of the Moon on 23 August 2023.\\nRussian lunar lander Luna 25 was launched on 10 August 2023, 23:10 UTC, atop a Soyuz-2.1b rocket from the Vostochny Cosmodrome, it was the first Russian attempt to land a spacecraft on the Moon since the Soviet lander Luna 24 in 1974, it crashed on the Moon on 19 August after technical glitches.\\n\\n  Q: Tell me about the Japanese lunar mission.\\n  A:\\n  '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_augmented_prompt = create_augmented_prompt(\n",
    "    \"Tell me about the Japanese lunar mission.\", \n",
    "    example_retrieved_docs\n",
    ")\n",
    "example_augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08bc45-09d6-41fa-b2ba-c3312b0db48d",
   "metadata": {},
   "source": [
    "The last important piece is querying the LLM itself. This is simple enough. Here is another simple function to query the OpenAI GPT-4 API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3b57eb-7699-4b7a-817e-9a1298b888a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def generate_response(query_string, model_name):\n",
    "  response = openai.ChatCompletion.create(\n",
    "      model=model_name, messages=[\n",
    "          {\"content\": query_string, \"role\": \"user\"}\n",
    "      ]\n",
    "  )\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0558fea-2a1a-4db0-965b-1f3eeca887f6",
   "metadata": {},
   "source": [
    "Once again, you can see how this behaves with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22753600-4ddf-4a0c-82a3-6729097ac816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Hello, world!\", model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83ae48-f06c-4348-a920-075738a399bb",
   "metadata": {},
   "source": [
    "Okay, you've now seen all of the core components of creating using RAG to query an LLM. Time to bring it all together! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a372e7b-5006-4c3d-bd50-1d4b3617c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_response(\n",
    "    query_string, \n",
    "    docs, \n",
    "    doc_index, \n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    k=3\n",
    "):\n",
    "\n",
    "  # R: retrieve documents\n",
    "  retrieved_docs = retrieve_documents(\n",
    "      query_string, doc_index, documents\n",
    "  )\n",
    "  # A: create augmented prompt\n",
    "  augmented_prompt = create_augmented_prompt(query_string, retrieved_docs)\n",
    "\n",
    "  # G: generate response!\n",
    "  generated_response = generate_response(augmented_prompt, model_name)\n",
    "  return generated_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4645f5-7a70-4752-9358-8bcd47ff8d5a",
   "metadata": {},
   "source": [
    "Now generate a RAG response with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d61cf4f-bd77-4fe9-8985-8e6d26f23c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The latest Indian lunar mission, Chandrayaan-3, was launched by ISRO on 14 July 2023. It consists of a lander, rover, and a propulsion module. The mission successfully landed in the south pole region of the Moon on 23 August 2023.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_rag_response(\"Tell me about the status of the latest Indian lunar mission.\", documents, document_index, model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170342e-7366-4066-961f-13dc693cb7b0",
   "metadata": {},
   "source": [
    "Up-to-date and accurate. Nice. Let's compare the RAG response to a 'raw' GPT-4 response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76062090-c246-4024-9178-a5a0d70a9c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of March 2022, the latest Indian lunar mission was Chandrayaan-3. The mission is being led by the Indian Space Research Organisation (ISRO) and is planned for launch in 2022. After a previous attempt failed to make a soft landing on the moon with Chandrayaan-2 in September 2019, ISRO has been preparing to retry the mission without an orbiter. The mission will include a lander and a rover to study the lunar surface close to the south pole, a largely unexplored area. However, for detailed real-time status, it would be best to refer to official ISRO channels or updates.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Tell me about the status of the latest Indian lunar mission.\", model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e0788-0b88-45fa-84c7-ff6bdf5e4e6b",
   "metadata": {},
   "source": [
    "This response is clearly relying on out-of-date information!\n",
    "\n",
    "## Next steps\n",
    "\n",
    "This is clearly a very simple example. A production system inevitably comes with a bit more complexity. For one thing, using a good vector database will likely help make your RAG setup faster, more scalable and more flexible. Here are some pointers on what to consider when taking the next step:\n",
    "\n",
    "* **Prompt engineering** - The prompt in this notebook is about as basic as it gets. Careful prompt engineering can make it possible for the LLM to directly reference specific retrieved documents, and to add hyperlinks and other interactivity into the response. You can also get the LLM to respond in structured formats like JSON, too. The [Prompting Guide](https://www.promptingguide.ai/techniques/rag) is a great resource to help get started with this.\n",
    "* **Multi-index retrieval** - In this case, you used a single document index. It is not uncommon to need to use multiple document data stores. Many vector databases make it easy to achieve this.\n",
    "* **Fine-tuning RAG models** - The [original RAG paper](https://arxiv.org/pdf/2005.11401.pdf) discusses how to train an embedding model (i.e. the model used for retrieval task) alongside the generation model (i.e. the language model) to perform better at certain tasks. OpenAI themselves provide a 'cookbook' notebook outlining how you can [fine tune an LLM to produce better RAG responses](https://cookbook.openai.com/examples/fine-tuned_qa/ft_retrieval_augmented_generation_qdrant).\n",
    "* **Caching** - Thoughtful use of caching layers around the retriever (and possibly the generator) can improve the latency and cost efficiency of your RAG system.\n",
    "\n",
    "If you're also a bit unfamiliar with embeddings, a great use of your time would be to learn more about how they fit into the world of modern machine learning. There's [a good introduction in the Google ML Developers course](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture). If you're a visual learner, you may also want to check out the interactive [Embedding Projector](https://projector.tensorflow.org/) to visualise embeddings, too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-explorations",
   "language": "python",
   "name": "llm-explorations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
